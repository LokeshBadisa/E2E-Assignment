{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from finetune import *\n",
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & create dataloaders\n",
    "def load_data():\n",
    "    '''\n",
    "    Different from the one in finetune.py, this function loads test data only\n",
    "    '''\n",
    "    data = h5py.File('Dataset_Specific_labelled.h5', 'r')  \n",
    "    X = data['jet'][...]\n",
    "    y = data['Y'][...]\n",
    "    y = np.squeeze(y)\n",
    "    X = rearrange(X, 'b h w c -> b c h w')\n",
    "    yt = np.zeros((y.shape[0],2))\n",
    "    yt[:,0] = 1 - y\n",
    "    yt[:,1] = y\n",
    "    y = yt\n",
    "    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)    \n",
    "    testset = TensorDataset(torch.tensor(X_test,dtype=torch.float32), torch.tensor(y_test,dtype=torch.float32))\n",
    "    testloader = DataLoader(testset, batch_size=512, shuffle=True)\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTunedModel(\n",
       "  (model): ViTMAE(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(8, 256, kernel_size=(5, 5), stride=(5, 5))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder_embed): Identity()\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=128, out_features=200, bias=True)\n",
       "  )\n",
       "  (mask_token): Identity()\n",
       "  (decoder_pos_embed): Identity()\n",
       "  (decoder_blocks): Identity()\n",
       "  (decoder_norm): Identity()\n",
       "  (decoder_pred): Identity()\n",
       "  (fc): Linear(in_features=160256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = FineTunedModel()\n",
    "model.load_state_dict(torch.load('weights.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "testloader = load_data()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  78.45 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "acc, _ = test_model(testloader, model, criterion, device)\n",
    "print(\"Accuracy: \", acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can pass the test cases by using the following code. \n",
    "```python\n",
    "var = ...\n",
    "out = model(var)\n",
    "out.argmax(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 25, 25]          51,456\n",
      "          Identity-2             [-1, 625, 256]               0\n",
      "        PatchEmbed-3             [-1, 625, 256]               0\n",
      "         LayerNorm-4             [-1, 626, 256]             512\n",
      "            Linear-5             [-1, 626, 768]         197,376\n",
      "          Identity-6           [-1, 4, 626, 64]               0\n",
      "          Identity-7           [-1, 4, 626, 64]               0\n",
      "            Linear-8             [-1, 626, 256]          65,792\n",
      "           Dropout-9             [-1, 626, 256]               0\n",
      "        Attention-10             [-1, 626, 256]               0\n",
      "         Identity-11             [-1, 626, 256]               0\n",
      "         Identity-12             [-1, 626, 256]               0\n",
      "        LayerNorm-13             [-1, 626, 256]             512\n",
      "           Linear-14            [-1, 626, 1024]         263,168\n",
      "             GELU-15            [-1, 626, 1024]               0\n",
      "          Dropout-16            [-1, 626, 1024]               0\n",
      "         Identity-17            [-1, 626, 1024]               0\n",
      "           Linear-18             [-1, 626, 256]         262,400\n",
      "          Dropout-19             [-1, 626, 256]               0\n",
      "              Mlp-20             [-1, 626, 256]               0\n",
      "         Identity-21             [-1, 626, 256]               0\n",
      "         Identity-22             [-1, 626, 256]               0\n",
      "            Block-23             [-1, 626, 256]               0\n",
      "        LayerNorm-24             [-1, 626, 256]             512\n",
      "           Linear-25             [-1, 626, 768]         197,376\n",
      "         Identity-26           [-1, 4, 626, 64]               0\n",
      "         Identity-27           [-1, 4, 626, 64]               0\n",
      "           Linear-28             [-1, 626, 256]          65,792\n",
      "          Dropout-29             [-1, 626, 256]               0\n",
      "        Attention-30             [-1, 626, 256]               0\n",
      "         Identity-31             [-1, 626, 256]               0\n",
      "         Identity-32             [-1, 626, 256]               0\n",
      "        LayerNorm-33             [-1, 626, 256]             512\n",
      "           Linear-34            [-1, 626, 1024]         263,168\n",
      "             GELU-35            [-1, 626, 1024]               0\n",
      "          Dropout-36            [-1, 626, 1024]               0\n",
      "         Identity-37            [-1, 626, 1024]               0\n",
      "           Linear-38             [-1, 626, 256]         262,400\n",
      "          Dropout-39             [-1, 626, 256]               0\n",
      "              Mlp-40             [-1, 626, 256]               0\n",
      "         Identity-41             [-1, 626, 256]               0\n",
      "         Identity-42             [-1, 626, 256]               0\n",
      "            Block-43             [-1, 626, 256]               0\n",
      "        LayerNorm-44             [-1, 626, 256]             512\n",
      "           Linear-45             [-1, 626, 768]         197,376\n",
      "         Identity-46           [-1, 4, 626, 64]               0\n",
      "         Identity-47           [-1, 4, 626, 64]               0\n",
      "           Linear-48             [-1, 626, 256]          65,792\n",
      "          Dropout-49             [-1, 626, 256]               0\n",
      "        Attention-50             [-1, 626, 256]               0\n",
      "         Identity-51             [-1, 626, 256]               0\n",
      "         Identity-52             [-1, 626, 256]               0\n",
      "        LayerNorm-53             [-1, 626, 256]             512\n",
      "           Linear-54            [-1, 626, 1024]         263,168\n",
      "             GELU-55            [-1, 626, 1024]               0\n",
      "          Dropout-56            [-1, 626, 1024]               0\n",
      "         Identity-57            [-1, 626, 1024]               0\n",
      "           Linear-58             [-1, 626, 256]         262,400\n",
      "          Dropout-59             [-1, 626, 256]               0\n",
      "              Mlp-60             [-1, 626, 256]               0\n",
      "         Identity-61             [-1, 626, 256]               0\n",
      "         Identity-62             [-1, 626, 256]               0\n",
      "            Block-63             [-1, 626, 256]               0\n",
      "        LayerNorm-64             [-1, 626, 256]             512\n",
      "           Linear-65             [-1, 626, 768]         197,376\n",
      "         Identity-66           [-1, 4, 626, 64]               0\n",
      "         Identity-67           [-1, 4, 626, 64]               0\n",
      "           Linear-68             [-1, 626, 256]          65,792\n",
      "          Dropout-69             [-1, 626, 256]               0\n",
      "        Attention-70             [-1, 626, 256]               0\n",
      "         Identity-71             [-1, 626, 256]               0\n",
      "         Identity-72             [-1, 626, 256]               0\n",
      "        LayerNorm-73             [-1, 626, 256]             512\n",
      "           Linear-74            [-1, 626, 1024]         263,168\n",
      "             GELU-75            [-1, 626, 1024]               0\n",
      "          Dropout-76            [-1, 626, 1024]               0\n",
      "         Identity-77            [-1, 626, 1024]               0\n",
      "           Linear-78             [-1, 626, 256]         262,400\n",
      "          Dropout-79             [-1, 626, 256]               0\n",
      "              Mlp-80             [-1, 626, 256]               0\n",
      "         Identity-81             [-1, 626, 256]               0\n",
      "         Identity-82             [-1, 626, 256]               0\n",
      "            Block-83             [-1, 626, 256]               0\n",
      "        LayerNorm-84             [-1, 626, 256]             512\n",
      "           Linear-85             [-1, 626, 768]         197,376\n",
      "         Identity-86           [-1, 4, 626, 64]               0\n",
      "         Identity-87           [-1, 4, 626, 64]               0\n",
      "           Linear-88             [-1, 626, 256]          65,792\n",
      "          Dropout-89             [-1, 626, 256]               0\n",
      "        Attention-90             [-1, 626, 256]               0\n",
      "         Identity-91             [-1, 626, 256]               0\n",
      "         Identity-92             [-1, 626, 256]               0\n",
      "        LayerNorm-93             [-1, 626, 256]             512\n",
      "           Linear-94            [-1, 626, 1024]         263,168\n",
      "             GELU-95            [-1, 626, 1024]               0\n",
      "          Dropout-96            [-1, 626, 1024]               0\n",
      "         Identity-97            [-1, 626, 1024]               0\n",
      "           Linear-98             [-1, 626, 256]         262,400\n",
      "          Dropout-99             [-1, 626, 256]               0\n",
      "             Mlp-100             [-1, 626, 256]               0\n",
      "        Identity-101             [-1, 626, 256]               0\n",
      "        Identity-102             [-1, 626, 256]               0\n",
      "           Block-103             [-1, 626, 256]               0\n",
      "       LayerNorm-104             [-1, 626, 256]             512\n",
      "          Linear-105             [-1, 626, 768]         197,376\n",
      "        Identity-106           [-1, 4, 626, 64]               0\n",
      "        Identity-107           [-1, 4, 626, 64]               0\n",
      "          Linear-108             [-1, 626, 256]          65,792\n",
      "         Dropout-109             [-1, 626, 256]               0\n",
      "       Attention-110             [-1, 626, 256]               0\n",
      "        Identity-111             [-1, 626, 256]               0\n",
      "        Identity-112             [-1, 626, 256]               0\n",
      "       LayerNorm-113             [-1, 626, 256]             512\n",
      "          Linear-114            [-1, 626, 1024]         263,168\n",
      "            GELU-115            [-1, 626, 1024]               0\n",
      "         Dropout-116            [-1, 626, 1024]               0\n",
      "        Identity-117            [-1, 626, 1024]               0\n",
      "          Linear-118             [-1, 626, 256]         262,400\n",
      "         Dropout-119             [-1, 626, 256]               0\n",
      "             Mlp-120             [-1, 626, 256]               0\n",
      "        Identity-121             [-1, 626, 256]               0\n",
      "        Identity-122             [-1, 626, 256]               0\n",
      "           Block-123             [-1, 626, 256]               0\n",
      "       LayerNorm-124             [-1, 626, 256]             512\n",
      "          Linear-125             [-1, 626, 768]         197,376\n",
      "        Identity-126           [-1, 4, 626, 64]               0\n",
      "        Identity-127           [-1, 4, 626, 64]               0\n",
      "          Linear-128             [-1, 626, 256]          65,792\n",
      "         Dropout-129             [-1, 626, 256]               0\n",
      "       Attention-130             [-1, 626, 256]               0\n",
      "        Identity-131             [-1, 626, 256]               0\n",
      "        Identity-132             [-1, 626, 256]               0\n",
      "       LayerNorm-133             [-1, 626, 256]             512\n",
      "          Linear-134            [-1, 626, 1024]         263,168\n",
      "            GELU-135            [-1, 626, 1024]               0\n",
      "         Dropout-136            [-1, 626, 1024]               0\n",
      "        Identity-137            [-1, 626, 1024]               0\n",
      "          Linear-138             [-1, 626, 256]         262,400\n",
      "         Dropout-139             [-1, 626, 256]               0\n",
      "             Mlp-140             [-1, 626, 256]               0\n",
      "        Identity-141             [-1, 626, 256]               0\n",
      "        Identity-142             [-1, 626, 256]               0\n",
      "           Block-143             [-1, 626, 256]               0\n",
      "       LayerNorm-144             [-1, 626, 256]             512\n",
      "          Linear-145             [-1, 626, 768]         197,376\n",
      "        Identity-146           [-1, 4, 626, 64]               0\n",
      "        Identity-147           [-1, 4, 626, 64]               0\n",
      "          Linear-148             [-1, 626, 256]          65,792\n",
      "         Dropout-149             [-1, 626, 256]               0\n",
      "       Attention-150             [-1, 626, 256]               0\n",
      "        Identity-151             [-1, 626, 256]               0\n",
      "        Identity-152             [-1, 626, 256]               0\n",
      "       LayerNorm-153             [-1, 626, 256]             512\n",
      "          Linear-154            [-1, 626, 1024]         263,168\n",
      "            GELU-155            [-1, 626, 1024]               0\n",
      "         Dropout-156            [-1, 626, 1024]               0\n",
      "        Identity-157            [-1, 626, 1024]               0\n",
      "          Linear-158             [-1, 626, 256]         262,400\n",
      "         Dropout-159             [-1, 626, 256]               0\n",
      "             Mlp-160             [-1, 626, 256]               0\n",
      "        Identity-161             [-1, 626, 256]               0\n",
      "        Identity-162             [-1, 626, 256]               0\n",
      "           Block-163             [-1, 626, 256]               0\n",
      "       LayerNorm-164             [-1, 626, 256]             512\n",
      "          Linear-165             [-1, 626, 768]         197,376\n",
      "        Identity-166           [-1, 4, 626, 64]               0\n",
      "        Identity-167           [-1, 4, 626, 64]               0\n",
      "          Linear-168             [-1, 626, 256]          65,792\n",
      "         Dropout-169             [-1, 626, 256]               0\n",
      "       Attention-170             [-1, 626, 256]               0\n",
      "        Identity-171             [-1, 626, 256]               0\n",
      "        Identity-172             [-1, 626, 256]               0\n",
      "       LayerNorm-173             [-1, 626, 256]             512\n",
      "          Linear-174            [-1, 626, 1024]         263,168\n",
      "            GELU-175            [-1, 626, 1024]               0\n",
      "         Dropout-176            [-1, 626, 1024]               0\n",
      "        Identity-177            [-1, 626, 1024]               0\n",
      "          Linear-178             [-1, 626, 256]         262,400\n",
      "         Dropout-179             [-1, 626, 256]               0\n",
      "             Mlp-180             [-1, 626, 256]               0\n",
      "        Identity-181             [-1, 626, 256]               0\n",
      "        Identity-182             [-1, 626, 256]               0\n",
      "           Block-183             [-1, 626, 256]               0\n",
      "       LayerNorm-184             [-1, 626, 256]             512\n",
      "          Linear-185             [-1, 626, 768]         197,376\n",
      "        Identity-186           [-1, 4, 626, 64]               0\n",
      "        Identity-187           [-1, 4, 626, 64]               0\n",
      "          Linear-188             [-1, 626, 256]          65,792\n",
      "         Dropout-189             [-1, 626, 256]               0\n",
      "       Attention-190             [-1, 626, 256]               0\n",
      "        Identity-191             [-1, 626, 256]               0\n",
      "        Identity-192             [-1, 626, 256]               0\n",
      "       LayerNorm-193             [-1, 626, 256]             512\n",
      "          Linear-194            [-1, 626, 1024]         263,168\n",
      "            GELU-195            [-1, 626, 1024]               0\n",
      "         Dropout-196            [-1, 626, 1024]               0\n",
      "        Identity-197            [-1, 626, 1024]               0\n",
      "          Linear-198             [-1, 626, 256]         262,400\n",
      "         Dropout-199             [-1, 626, 256]               0\n",
      "             Mlp-200             [-1, 626, 256]               0\n",
      "        Identity-201             [-1, 626, 256]               0\n",
      "        Identity-202             [-1, 626, 256]               0\n",
      "           Block-203             [-1, 626, 256]               0\n",
      "       LayerNorm-204             [-1, 626, 256]             512\n",
      "          Linear-205             [-1, 626, 768]         197,376\n",
      "        Identity-206           [-1, 4, 626, 64]               0\n",
      "        Identity-207           [-1, 4, 626, 64]               0\n",
      "          Linear-208             [-1, 626, 256]          65,792\n",
      "         Dropout-209             [-1, 626, 256]               0\n",
      "       Attention-210             [-1, 626, 256]               0\n",
      "        Identity-211             [-1, 626, 256]               0\n",
      "        Identity-212             [-1, 626, 256]               0\n",
      "       LayerNorm-213             [-1, 626, 256]             512\n",
      "          Linear-214            [-1, 626, 1024]         263,168\n",
      "            GELU-215            [-1, 626, 1024]               0\n",
      "         Dropout-216            [-1, 626, 1024]               0\n",
      "        Identity-217            [-1, 626, 1024]               0\n",
      "          Linear-218             [-1, 626, 256]         262,400\n",
      "         Dropout-219             [-1, 626, 256]               0\n",
      "             Mlp-220             [-1, 626, 256]               0\n",
      "        Identity-221             [-1, 626, 256]               0\n",
      "        Identity-222             [-1, 626, 256]               0\n",
      "           Block-223             [-1, 626, 256]               0\n",
      "       LayerNorm-224             [-1, 626, 256]             512\n",
      "          Linear-225             [-1, 626, 768]         197,376\n",
      "        Identity-226           [-1, 4, 626, 64]               0\n",
      "        Identity-227           [-1, 4, 626, 64]               0\n",
      "          Linear-228             [-1, 626, 256]          65,792\n",
      "         Dropout-229             [-1, 626, 256]               0\n",
      "       Attention-230             [-1, 626, 256]               0\n",
      "        Identity-231             [-1, 626, 256]               0\n",
      "        Identity-232             [-1, 626, 256]               0\n",
      "       LayerNorm-233             [-1, 626, 256]             512\n",
      "          Linear-234            [-1, 626, 1024]         263,168\n",
      "            GELU-235            [-1, 626, 1024]               0\n",
      "         Dropout-236            [-1, 626, 1024]               0\n",
      "        Identity-237            [-1, 626, 1024]               0\n",
      "          Linear-238             [-1, 626, 256]         262,400\n",
      "         Dropout-239             [-1, 626, 256]               0\n",
      "             Mlp-240             [-1, 626, 256]               0\n",
      "        Identity-241             [-1, 626, 256]               0\n",
      "        Identity-242             [-1, 626, 256]               0\n",
      "           Block-243             [-1, 626, 256]               0\n",
      "       LayerNorm-244             [-1, 626, 256]             512\n",
      "          Linear-245                    [-1, 2]         320,514\n",
      "================================================================\n",
      "Total params: 9,849,602\n",
      "Trainable params: 9,849,602\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.48\n",
      "Forward/backward pass size (MB): 503.73\n",
      "Params size (MB): 37.57\n",
      "Estimated Total Size (MB): 541.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (8, 125, 125))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lokesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
